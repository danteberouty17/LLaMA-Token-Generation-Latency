{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5dce50-7b87-48a3-8dde-2af2c9bbc4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: torch in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.9.8)\n",
      "Requirement already satisfied: pynvml in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (13.0.1)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ollama) (2.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.3.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: nvidia-ml-py>=12.0.0 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pynvml) (13.580.82)\n",
      "Requirement already satisfied: anyio in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.3)\n",
      "Requirement already satisfied: idna in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama) (3.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\chees\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ollama torch pandas matplotlib tqdm psutil pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4c4acb-a833-4ecd-940e-adcc6f42453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup stuff for confirming directory pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e66a36a-d690-4ccb-8051-f41e2c14b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt directory: C:\\Users\\chees\\CECS 530 Project\\prompts\n",
      "Results will be saved to: C:\\Users\\chees\\CECS 530 Project\\results\\3060_llama_latency.csv\n",
      "simple_short 128 prompts loaded\n",
      "simple_long 128 prompts loaded\n",
      "moderate_short 128 prompts loaded\n",
      "moderate_long 128 prompts loaded\n",
      "complex_short 128 prompts loaded\n",
      "complex_long 128 prompts loaded\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import ollama \n",
    "\n",
    "# Directory setup \n",
    "BASE_DIR = Path.cwd()\n",
    "PROMPT_DIR = BASE_DIR / \"prompts\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "HOMOGENEOUS_CSV = RESULTS_DIR / \"3060_llama_latency.csv\"\n",
    "MIXED_CSV = RESULTS_DIR / \"llama_latency_mixed.csv\"\n",
    "\n",
    "BATCH_SUMMARY_HOMOGENEOUS = RESULTS_DIR / \"3060_llama_batch_summary.csv\"\n",
    "BATCH_SUMMARY_MIXED = RESULTS_DIR / \"llama_batch_summary_mixed.csv\"\n",
    "\n",
    "\n",
    "# CONFIGURATION FOR MIXED PROMPTS VS HOMOGENEOUS\n",
    "\n",
    "# For a mixed batch (all prompt types), \"real world\" workload \n",
    "# csv_file = MIXED_CSV\n",
    "# csv_type = \"mixed\"\n",
    "# homogeneous_key = None  # Not used for mixed runs\n",
    "\n",
    "# For homogeneous batch (only one type/length), experiment workload \n",
    "csv_file = HOMOGENEOUS_CSV\n",
    "csv_type = \"homogeneous\"\n",
    "homogeneous_key = \"simple_short\"  # e.g., simple_short, moderate_long, etc.\n",
    "\n",
    "print(\"Prompt directory:\", PROMPT_DIR)\n",
    "print(\"Results will be saved to:\", csv_file)\n",
    "\n",
    "# Load prompts \n",
    "prompt_files = {\n",
    "    \"simple_short\": PROMPT_DIR / \"simple_short.txt\",\n",
    "    \"simple_long\": PROMPT_DIR / \"simple_long.txt\",\n",
    "    \"moderate_short\": PROMPT_DIR / \"moderate_short.txt\",\n",
    "    \"moderate_long\": PROMPT_DIR / \"moderate_long.txt\",\n",
    "    \"complex_short\": PROMPT_DIR / \"complex_short.txt\", \n",
    "    \"complex_long\": PROMPT_DIR / \"complex_long.txt\" \n",
    "}\n",
    "\n",
    "prompts = {}\n",
    "for key, file_path in prompt_files.items():\n",
    "    if file_path.exists():\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            prompts[key] = [line.strip() for line in f if line.strip()]\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        prompts[key] = []\n",
    "\n",
    "# Quick check\n",
    "for k, v in prompts.items():\n",
    "    print(k, len(v), \"prompts loaded\")\n",
    "\n",
    "# Prompt bank for storage\n",
    "prompt_bank = prompts\n",
    "\n",
    "# Keys for running through tests on all key types, for overnight runs\n",
    "homogeneous_keys = [\n",
    "    \"simple_short\",\n",
    "    \"simple_long\",\n",
    "    \"moderate_short\",\n",
    "    \"moderate_long\", \n",
    "    \"complex_short\",\n",
    "    \"complex_long\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "575644b8-10e4-4fab-9572-fc671ffc22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking code with a warmup cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab19eca-660a-4bfa-ab89-f36685ecb189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warming up model (this may take a few seconds)...\n",
      "Warm-up complete in 7.09 seconds.\n",
      "\n",
      "=== Running batch size 1 ===\n",
      "Batch size 1 finished in 0.95s with 1 prompts.\n",
      "\n",
      "=== Running batch size 8 ===\n",
      "Batch size 8 finished in 11.31s with 8 prompts.\n",
      "\n",
      "=== Running batch size 32 ===\n",
      "Batch size 32 finished in 24.36s with 32 prompts.\n",
      "\n",
      "=== Running batch size 128 ===\n",
      "Batch size 128 finished in 123.28s with 128 prompts.\n",
      "\n",
      "Experiment completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>prompt_eval_time</th>\n",
       "      <th>output_eval_time</th>\n",
       "      <th>total_time</th>\n",
       "      <th>avg_token_latency</th>\n",
       "      <th>throughput</th>\n",
       "      <th>model_name</th>\n",
       "      <th>gpu_name</th>\n",
       "      <th>gpu_mem_used_MB</th>\n",
       "      <th>gpu_mem_total_MB</th>\n",
       "      <th>gpu_util_percent</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-05T11:24:46.445971</td>\n",
       "      <td>simple</td>\n",
       "      <td>short</td>\n",
       "      <td>7</td>\n",
       "      <td>62</td>\n",
       "      <td>0.146510</td>\n",
       "      <td>0.801081</td>\n",
       "      <td>0.947592</td>\n",
       "      <td>0.012921</td>\n",
       "      <td>77.395379</td>\n",
       "      <td>llama3</td>\n",
       "      <td>NVIDIA GeForce RTX 3060 Ti</td>\n",
       "      <td>6553</td>\n",
       "      <td>8192</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-05T11:24:46.859090</td>\n",
       "      <td>simple</td>\n",
       "      <td>short</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.196511</td>\n",
       "      <td>0.158616</td>\n",
       "      <td>0.355127</td>\n",
       "      <td>0.012201</td>\n",
       "      <td>81.958911</td>\n",
       "      <td>llama3</td>\n",
       "      <td>NVIDIA GeForce RTX 3060 Ti</td>\n",
       "      <td>6559</td>\n",
       "      <td>8192</td>\n",
       "      <td>94</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-05T11:24:46.979990</td>\n",
       "      <td>simple</td>\n",
       "      <td>short</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.394022</td>\n",
       "      <td>0.079504</td>\n",
       "      <td>0.473526</td>\n",
       "      <td>0.011358</td>\n",
       "      <td>88.046134</td>\n",
       "      <td>llama3</td>\n",
       "      <td>NVIDIA GeForce RTX 3060 Ti</td>\n",
       "      <td>6559</td>\n",
       "      <td>8192</td>\n",
       "      <td>94</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-05T11:24:47.122496</td>\n",
       "      <td>simple</td>\n",
       "      <td>short</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.501026</td>\n",
       "      <td>0.118505</td>\n",
       "      <td>0.619531</td>\n",
       "      <td>0.011851</td>\n",
       "      <td>84.384624</td>\n",
       "      <td>llama3</td>\n",
       "      <td>NVIDIA GeForce RTX 3060 Ti</td>\n",
       "      <td>6559</td>\n",
       "      <td>8192</td>\n",
       "      <td>94</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-05T11:24:47.277139</td>\n",
       "      <td>simple</td>\n",
       "      <td>short</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.641034</td>\n",
       "      <td>0.131141</td>\n",
       "      <td>0.772175</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>83.879217</td>\n",
       "      <td>llama3</td>\n",
       "      <td>NVIDIA GeForce RTX 3060 Ti</td>\n",
       "      <td>6559</td>\n",
       "      <td>8192</td>\n",
       "      <td>94</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp prompt_type prompt_length  input_tokens  \\\n",
       "0  2025-11-05T11:24:46.445971      simple         short             7   \n",
       "1  2025-11-05T11:24:46.859090      simple         short             7   \n",
       "2  2025-11-05T11:24:46.979990      simple         short             7   \n",
       "3  2025-11-05T11:24:47.122496      simple         short             7   \n",
       "4  2025-11-05T11:24:47.277139      simple         short             5   \n",
       "\n",
       "   output_tokens  prompt_eval_time  output_eval_time  total_time  \\\n",
       "0             62          0.146510          0.801081    0.947592   \n",
       "1             13          0.196511          0.158616    0.355127   \n",
       "2              7          0.394022          0.079504    0.473526   \n",
       "3             10          0.501026          0.118505    0.619531   \n",
       "4             11          0.641034          0.131141    0.772175   \n",
       "\n",
       "   avg_token_latency  throughput model_name                    gpu_name  \\\n",
       "0           0.012921   77.395379     llama3  NVIDIA GeForce RTX 3060 Ti   \n",
       "1           0.012201   81.958911     llama3  NVIDIA GeForce RTX 3060 Ti   \n",
       "2           0.011358   88.046134     llama3  NVIDIA GeForce RTX 3060 Ti   \n",
       "3           0.011851   84.384624     llama3  NVIDIA GeForce RTX 3060 Ti   \n",
       "4           0.011922   83.879217     llama3  NVIDIA GeForce RTX 3060 Ti   \n",
       "\n",
       "   gpu_mem_used_MB  gpu_mem_total_MB  gpu_util_percent  batch_size  \n",
       "0             6553              8192                93           1  \n",
       "1             6559              8192                94           8  \n",
       "2             6559              8192                94           8  \n",
       "3             6559              8192                94           8  \n",
       "4             6559              8192                94           8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runs logged: 169\n"
     ]
    }
   ],
   "source": [
    "# Warm-up function\n",
    "def warmup_model(model_name, prompt=\"Hello! This is a warmup run.\"):\n",
    "    print(\"\\nWarming up model (this may take a few seconds)...\")\n",
    "    start = time.time()\n",
    "    _ = ollama.chat(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Warm-up complete in {elapsed:.2f} seconds.\")\n",
    "\n",
    "# GPU stats function\n",
    "import subprocess, re\n",
    "def get_gpu_info():\n",
    "    \"\"\"Returns GPU name, memory used/total, and utilization %.\"\"\"\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.used,memory.total,utilization.gpu\", \"--format=csv,noheader,nounits\"],\n",
    "            encoding=\"utf-8\"\n",
    "        ).strip()\n",
    "        name, mem_used, mem_total, util = re.split(r',\\s*', output)\n",
    "        return {\n",
    "            \"gpu_name\": name,\n",
    "            \"gpu_mem_used_MB\": int(mem_used),\n",
    "            \"gpu_mem_total_MB\": int(mem_total),\n",
    "            \"gpu_util_percent\": int(util)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"Could not read GPU stats:\", e)\n",
    "        return {\n",
    "            \"gpu_name\": \"N/A\",\n",
    "            \"gpu_mem_used_MB\": 0,\n",
    "            \"gpu_mem_total_MB\": 0,\n",
    "            \"gpu_util_percent\": 0\n",
    "        }\n",
    "\n",
    "# Single prompt benchmark function\n",
    "def run_single_prompt(model_name, prompt, prompt_type, prompt_length):\n",
    "    start_total = time.time()\n",
    "    # Load time (minimal since Ollama auto-loads)\n",
    "    load_time_start = time.time()\n",
    "    load_time = time.time() - load_time_start\n",
    "\n",
    "    # Prompt evaluation + streaming output\n",
    "    prompt_eval_start = time.time()\n",
    "    stream = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    first_token_time = None\n",
    "    token_count = 0\n",
    "    for chunk in stream:\n",
    "        if \"message\" in chunk:\n",
    "            token_count += 1\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time()\n",
    "\n",
    "    prompt_eval_time = (first_token_time - prompt_eval_start) if first_token_time else 0\n",
    "    output_eval_time = (time.time() - first_token_time) if first_token_time else 0\n",
    "    total_time = time.time() - start_total\n",
    "\n",
    "    avg_token_latency = output_eval_time / max(token_count,1)\n",
    "    throughput = token_count / output_eval_time if output_eval_time>0 else 0\n",
    "\n",
    "    return {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"prompt_type\": prompt_type,\n",
    "        \"prompt_length\": prompt_length,\n",
    "        \"input_tokens\": len(prompt.split()),\n",
    "        \"output_tokens\": token_count,\n",
    "        \"prompt_eval_time\": prompt_eval_time,\n",
    "        \"output_eval_time\": output_eval_time,\n",
    "        \"total_time\": total_time,\n",
    "        \"avg_token_latency\": avg_token_latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"model_name\": model_name\n",
    "    }\n",
    "\n",
    "# Batch runner with GPU logging\n",
    "def run_batch(model_name, batch_size, prompt_bank, csv_type=\"mixed\", homogeneous_key=homogeneous_key):\n",
    "    results = []\n",
    "\n",
    "    # Sample prompts\n",
    "    selected_prompts = []\n",
    "\n",
    "    # For controlled experiment run throughs\n",
    "    if csv_type == \"homogeneous\" and homogeneous_key is not None:\n",
    "        # Only sample from the specified type/length key\n",
    "        plist = prompt_bank[homogeneous_key]\n",
    "        prompt_type, prompt_length = homogeneous_key.split(\"_\")\n",
    "        for prompt in random.sample(plist, k=min(len(plist), batch_size)):\n",
    "            selected_prompts.append((prompt, prompt_type, prompt_length))\n",
    "\n",
    "    # For simulated \"real world\" test scenarios\n",
    "    else: \n",
    "        for key, plist in prompt_bank.items():\n",
    "            prompt_type, prompt_length = key.split(\"_\")\n",
    "            for prompt in random.sample(plist, k=min(2, len(plist))):\n",
    "                selected_prompts.append((prompt, prompt_type, prompt_length))\n",
    "\n",
    "    # Duplicate to match batch size if needed\n",
    "    while len(selected_prompts) < batch_size:\n",
    "        selected_prompts.extend(random.sample(selected_prompts, k=min(len(selected_prompts), batch_size-len(selected_prompts))))\n",
    "    selected_prompts = selected_prompts[:batch_size]\n",
    "\n",
    "    # Run batch concurrently\n",
    "    start_batch = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
    "        futures = [\n",
    "            executor.submit(run_single_prompt, model_name, prompt, ptype, plen)\n",
    "            for (prompt, ptype, plen) in selected_prompts\n",
    "        ]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            results.append(future.result())\n",
    "    batch_time = time.time() - start_batch\n",
    "\n",
    "    # GPU stats\n",
    "    gpu_stats = get_gpu_info()\n",
    "    for r in results:\n",
    "        r.update({\n",
    "            \"gpu_name\": gpu_stats[\"gpu_name\"],\n",
    "            \"gpu_mem_used_MB\": gpu_stats[\"gpu_mem_used_MB\"],\n",
    "            \"gpu_mem_total_MB\": gpu_stats[\"gpu_mem_total_MB\"],\n",
    "            \"gpu_util_percent\": gpu_stats[\"gpu_util_percent\"]\n",
    "        })\n",
    "\n",
    "    # Batch summary\n",
    "    total_output_tokens = sum(r[\"output_tokens\"] for r in results)\n",
    "    total_time = batch_time\n",
    "    total_tokens = total_output_tokens\n",
    "\n",
    "    # Throughput = total tokens generated / total batch time\n",
    "    avg_throughput = total_tokens / total_time if total_time > 0 else 0\n",
    "\n",
    "    # Average token latency = total time / total tokens\n",
    "    avg_token_latency = total_time / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "\n",
    "    batch_summary = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"batch_size\": batch_size,\n",
    "        \"prompt_type\": prompt_type,\n",
    "        \"prompt_length\": prompt_length,\n",
    "        \"total_output_tokens\": total_output_tokens,\n",
    "        \"batch_time\": batch_time,\n",
    "        \"avg_token_latency\": avg_token_latency,\n",
    "        \"avg_throughput\": avg_throughput,\n",
    "        \"gpu_name\": gpu_stats[\"gpu_name\"],\n",
    "        \"gpu_mem_used_MB\": gpu_stats[\"gpu_mem_used_MB\"],\n",
    "        \"gpu_mem_total_MB\": gpu_stats[\"gpu_mem_total_MB\"],\n",
    "        \"gpu_util_percent\": gpu_stats[\"gpu_util_percent\"]\n",
    "    }\n",
    "\n",
    "    # Write batch summary to correct CSV\n",
    "    if csv_type == \"homogeneous\":\n",
    "        batch_csv_file = BATCH_SUMMARY_HOMOGENEOUS\n",
    "    else:\n",
    "        batch_csv_file = BATCH_SUMMARY_MIXED\n",
    "\n",
    "    pd.DataFrame([batch_summary]).to_csv(batch_csv_file, mode='a', header=not batch_csv_file.exists(), index=False)\n",
    "\n",
    "    print(f\"Batch size {batch_size} finished in {batch_time:.2f}s with {len(results)} prompts.\")\n",
    "    return results\n",
    "\n",
    "# Full experiment loop\n",
    "def run_full_experiment(model_name, batch_sizes, prompt_bank, csv_file, csv_type=\"mixed\", homogeneous_key=homogeneous_key):\n",
    "    all_results = []\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\n=== Running batch size {batch_size} ===\")\n",
    "        batch_results = run_batch(model_name, batch_size, prompt_bank, csv_type, homogeneous_key=homogeneous_key)\n",
    "        for r in batch_results:\n",
    "            r[\"batch_size\"] = batch_size\n",
    "        all_results.extend(batch_results)\n",
    "\n",
    "        # Incremental per-prompt CSV\n",
    "        pd.DataFrame(batch_results).to_csv(csv_file, mode='a', header=not csv_file.exists(), index=False)\n",
    "\n",
    "    print(\"\\nâœ… Experiment completed.\")\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "\n",
    "# BENCHMARKING PARAMETERS\n",
    "model_name = \"llama3\"  # change as needed\n",
    "batch_sizes = [1, 8, 32, 128]\n",
    "\n",
    "# Warm-up\n",
    "warmup_model(model_name)\n",
    "\n",
    "# Run full experiment, singular\n",
    "df_results = run_full_experiment(\n",
    "    model_name=model_name,\n",
    "    batch_sizes=batch_sizes,\n",
    "    prompt_bank=prompt_bank,\n",
    "    csv_file=csv_file,\n",
    "    csv_type=csv_type,\n",
    "    homogeneous_key=homogeneous_key\n",
    ")\n",
    "\n",
    "# Multiloop experiment \n",
    "# Number of repeats per prompt type\n",
    "# num_repeats = 5   # adjust as needed\n",
    "\n",
    "#Loop over prompt types and run\n",
    "#for key in homogeneous_keys:\n",
    "#    print(f\"\\n=== Running homogeneous tests for {key} ===\\n\")\n",
    "#    for i in range(num_repeats):\n",
    "#        print(f\"Run {i+1}/{num_repeats} for {key}\")\n",
    "#        df_results = run_full_experiment(\n",
    "#            model_name=model_name,\n",
    "#            batch_sizes=batch_sizes,\n",
    "#            prompt_bank=prompt_bank,\n",
    "#            csv_file=csv_file,\n",
    "#            csv_type=\"homogeneous\",\n",
    "#            homogeneous_key=key\n",
    "#        )\n",
    "#        print(f\"Completed {key} run {i+1}/{num_repeats}\\n\")\n",
    "\n",
    "# Quick CSV preview\n",
    "display(df_results.head())\n",
    "print(f\"Total runs logged: {len(df_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da0cc4-ec9d-4e80-b7c4-4028d6e22604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
